# Multi-Layer Perceptron (MLP) Implementation

##  Project Overview
This project features a custom implementation of a **Multi-Layer Perceptron (MLP)**, a fundamental deep learning architecture. It was developed to explore how neural networks capture non-linear patterns through hidden layers and the backpropagation algorithm.

##  Technical Features
* **Neural Architecture:** Implementation of input, hidden, and output layers with adjustable neuron counts.
* **Backpropagation Algorithm:** Manual implementation of the chain rule to calculate gradients and update weights/biases.
* **Non-Linear Activations:** Used functions such as **ReLU** or **Sigmoid** to allow the network to learn complex data representations.
* **Optimization:** Gradient Descent optimization to minimize the loss function (Cross-Entropy).
* **Matrix Computations:** Highly efficient vectorized implementation using **NumPy**.

##  Tech Stack
* **Language:** Python 3.x
* **Environment:** Jupyter Notebook
* **Libraries:** NumPy, Matplotlib (for performance visualization).

##  Results
The notebook includes:
- **Training Logs:** Visualization of the loss decreasing over iterations.
- **Accuracy Metrics:** Evaluation of the model's performance on the test set.
- **Decision Boundaries:** (Optional) Visual representation of how the model separates different classes.
